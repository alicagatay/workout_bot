{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Implementation of the chatbot as a backend structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this file, you can find the implementation of the backend architecture of my software project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The backend structure of my project is designed using the Python programming language. There are many reasons that I chose Python as the backend language, mainly because of the easiness it provides while developing machine learning models and the library support of it(mainly for machine learning and artificial intelligence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Importing the libraries needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In addition to the language, I also used some libraries that is built for Python language to build the backend. Here are the libraries I used:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "I used the random library in order to randomize the x and y input arrays on the training and set data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "I used the JSON library, so that I could use my training dataset(intents.json file), to train the deep learning model that I created later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "I used the NumPy library in order to do some scientific computing and some matrix calculations in my project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/alicagatay/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/alicagatay/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/alicagatay/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "I used the NLTK package in order to be able to work with texts in the machine learning model that I developed later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "I used the Tensorflow library in order to build the deep learning model that I used while developing the machine learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "I used the Scikit-learn library in order to split the dataset into training and testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### OS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "I used the os library so that I can open diferent directories when I need to(for example, to open the training dataset inside another folder, to open the model inside another directory, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Preprocessing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The first thing I did before creating the machine learning model was to preprocess the dataset so that I can use it to train the model. In order to accomplish this, I first loaded the dataset inside the intents.json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "chatbotData = os.path.relpath('..//data')\n",
    "chatbotIntents = json.loads(open(chatbotData + '/intents.json').read())\n",
    "exerciseData = json.loads(open(chatbotData + '/list_of_all_exercises.json').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "After loading the dataset from intents.json file, I separated the the texts into 3 different arrays; words, documents and classes. I also created an array called ignoreLetters, in order to ignore all the punctuation marks from the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "chatbotWords = []\n",
    "chatbotClasses = []\n",
    "chatbotDocuments = []\n",
    "ignoreLetters = ['?', '!', '.', ',', \"'\", '\"']\n",
    "for intent in chatbotIntents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        word_list = nltk.word_tokenize(pattern)\n",
    "        chatbotWords.extend(word_list)\n",
    "        chatbotDocuments.append((word_list, intent['tag']))\n",
    "        if intent['tag'] not in chatbotClasses:\n",
    "            chatbotClasses.append(intent['tag'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "After creating these arrays, I lemmatised the words in the words array. I also used the ignoreLetters array to ignore some of the words that I used in my training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "chatbotLemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "for word in chatbotWords:\n",
    "    if word not in ignoreLetters:\n",
    "        word = chatbotLemmatizer.lemmatize(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "After lemmatising the words, I sorted the arrays words and classes in order to make sure that the words in the words array are in the same order as the words in the classes array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "chatbotWords = sorted(set(chatbotWords))\n",
    "chatbotClasses = sorted(set(chatbotClasses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "After the conversion, I used the words array to create a bag of words for every document out there and added those bag of words into an array that I later used to create the training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "chatbotData = []\n",
    "output_empty = [0] * len(chatbotClasses)\n",
    "\n",
    "for document in chatbotDocuments:\n",
    "    bag = []\n",
    "    wordPatterns = document[0]\n",
    "\n",
    "\n",
    "    for word in wordPatterns:\n",
    "        word = chatbotLemmatizer.lemmatize(word.lower())\n",
    "\n",
    "    for word in chatbotWords:\n",
    "        if word in wordPatterns:\n",
    "            bag.append(1)\n",
    "        else:\n",
    "            bag.append(0)\n",
    "\n",
    "    output_row = list(output_empty)\n",
    "    output_row[chatbotClasses.index(document[1])] = 1\n",
    "    chatbotData.append([bag, output_row])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "I then randomised the contents of the data array so that the order of the bag of words is randomised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "random.shuffle(chatbotData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "I then splitted the data array into training and testing arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "training_data, testing_data = train_test_split(chatbotData, test_size=0.1, random_state=25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "After that, just before creating the neural network model, I separated the training dataset into two different arrays called train_x and train_y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_9/tgxc64p92ks65f7thjwm8fwc0000gn/T/ipykernel_65655/2759127376.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  training_data = np.array(training_data)\n"
     ]
    }
   ],
   "source": [
    "training_data = np.array(training_data)\n",
    "train_x = list(training_data[:, 0])\n",
    "train_y = list(training_data[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Same for the testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_9/tgxc64p92ks65f7thjwm8fwc0000gn/T/ipykernel_65655/3973432232.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  testing_data = np.array(testing_data)\n"
     ]
    }
   ],
   "source": [
    "testing_data = np.array(testing_data)\n",
    "test_x = list(testing_data[:, 0])\n",
    "test_y = list(testing_data[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Creating the neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-07 16:27:58.587763: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-04-07 16:27:58.587872: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2022-04-07 16:27:58.671046: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n",
      "Epoch 1/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.9889 - accuracy: 0.0691\n",
      "Epoch 2/100\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2.7966 - accuracy: 0.1493"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-07 16:27:58.847968: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 9ms/step - loss: 2.7359 - accuracy: 0.1600\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 2.5369 - accuracy: 0.1709\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 2.2252 - accuracy: 0.2873\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1.9577 - accuracy: 0.3745\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1.5599 - accuracy: 0.5527\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1.3434 - accuracy: 0.6000\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1.1842 - accuracy: 0.6145\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.8687 - accuracy: 0.7600\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.7171 - accuracy: 0.7964\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.6840 - accuracy: 0.7891\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.5361 - accuracy: 0.8182\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.3949 - accuracy: 0.9055\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.3830 - accuracy: 0.8909\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.3200 - accuracy: 0.9200\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.2560 - accuracy: 0.9309\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.1889 - accuracy: 0.9636\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.1673 - accuracy: 0.9564\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.1361 - accuracy: 0.9636\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0917 - accuracy: 0.9891\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.1403 - accuracy: 0.9491\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0879 - accuracy: 0.9745\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0809 - accuracy: 0.9818\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0694 - accuracy: 0.9855\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0817 - accuracy: 0.9818\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0648 - accuracy: 0.9891\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0740 - accuracy: 0.9855\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0853 - accuracy: 0.9818\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0593 - accuracy: 0.9891\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0443 - accuracy: 0.9927\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0637 - accuracy: 0.9818\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0339 - accuracy: 0.9964\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0429 - accuracy: 0.9891\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0242 - accuracy: 1.0000\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0305 - accuracy: 0.9891\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0370 - accuracy: 0.9964\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0252 - accuracy: 0.9891\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0293 - accuracy: 0.9964\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0210 - accuracy: 0.9964\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0206 - accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0179 - accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0110 - accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0282 - accuracy: 0.9927\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0141 - accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0332 - accuracy: 0.9891\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0170 - accuracy: 0.9964\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0161 - accuracy: 0.9964\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0272 - accuracy: 0.9927\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0196 - accuracy: 0.9964\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0207 - accuracy: 0.9927\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0180 - accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0196 - accuracy: 1.0000\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0276 - accuracy: 0.9927\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0094 - accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0148 - accuracy: 0.9964\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0158 - accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0069 - accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0149 - accuracy: 0.9964\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0169 - accuracy: 0.9964\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0077 - accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0076 - accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0263 - accuracy: 0.9927\n",
      "Epoch 64/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0211 - accuracy: 0.9964\n",
      "Epoch 65/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0237 - accuracy: 0.9927\n",
      "Epoch 67/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0147 - accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0080 - accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0169 - accuracy: 0.9964\n",
      "Epoch 70/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0267 - accuracy: 0.9927\n",
      "Epoch 71/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0080 - accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0117 - accuracy: 0.9964\n",
      "Epoch 73/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0113 - accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0116 - accuracy: 0.9964\n",
      "Epoch 75/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0137 - accuracy: 0.9927\n",
      "Epoch 76/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0131 - accuracy: 0.9964\n",
      "Epoch 77/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0059 - accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0118 - accuracy: 0.9964\n",
      "Epoch 79/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0061 - accuracy: 0.9964\n",
      "Epoch 80/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0065 - accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0087 - accuracy: 0.9964\n",
      "Epoch 82/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0326 - accuracy: 0.9964\n",
      "Epoch 83/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0054 - accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0096 - accuracy: 0.9964\n",
      "Epoch 85/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0098 - accuracy: 0.9964\n",
      "Epoch 88/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0038 - accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0104 - accuracy: 0.9964\n",
      "Epoch 91/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0118 - accuracy: 0.9964\n",
      "Epoch 92/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0067 - accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0043 - accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0272 - accuracy: 0.9891\n",
      "Epoch 95/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0067 - accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0123 - accuracy: 0.9927\n",
      "Epoch 97/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0070 - accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0158 - accuracy: 0.9964\n",
      "Epoch 99/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0127 - accuracy: 0.9927\n",
      "Epoch 100/100\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0112 - accuracy: 0.9964\n"
     ]
    }
   ],
   "source": [
    "chatbotModel = tf.keras.models.Sequential()\n",
    "chatbotModel.add(tf.keras.layers.Dense(len(train_x[0]), input_shape=(len(train_x[0]),), activation='relu'))\n",
    "chatbotModel.add(tf.keras.layers.Dropout(0.5))\n",
    "chatbotModel.add(tf.keras.layers.Dense(len(train_x[0])/2*3, activation='relu'))\n",
    "chatbotModel.add(tf.keras.layers.Dropout(0.5))\n",
    "chatbotModel.add(tf.keras.layers.Dense(len(train_y[0]), activation='softmax'))\n",
    "\n",
    "\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "chatbotModel.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "model_train = chatbotModel.fit(np.array(train_x), np.array(train_y), epochs=100, batch_size=len(train_x[0]), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 90ms/step - loss: 0.2500 - accuracy: 0.9677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-07 16:28:03.698199: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "model_test = chatbotModel.evaluate(np.array(test_x), np.array(test_y), verbose=1, batch_size=len(test_x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "models = os.path.relpath('..//models')\n",
    "chatbotModel.save(models + '/chatbot_model.h5', model_train)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining those variables, I started to implement some functions that I will use in the chatbot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first one is called \"clean_up_sentence\". This function is used to clean up the user's input so that it can be converted to a computable state for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def clean_up_sentence(sentence):\n",
    "    sentenceWords = nltk.word_tokenize(sentence)\n",
    "\n",
    "    for word in sentenceWords:\n",
    "        word = chatbotLemmatizer.lemmatize(word.lower())\n",
    "        \n",
    "    return sentenceWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second one is called \"bag_of_words\". This function is used to create a bag of words for the user's input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(sentence):\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    bag = [0] * len(chatbotWords)\n",
    "    for w in sentence_words:\n",
    "        for i, word in enumerate(chatbotWords):\n",
    "            if word == w:\n",
    "                bag[i] = 1\n",
    "    return np.array(bag)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other function that I created is called \"predict_class\". This function is used to predict a valid response for the user's input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class(sentence):\n",
    "    bow = bag_of_words(sentence)\n",
    "    res = chatbotModel.predict(np.array([bow]))[0]\n",
    "    error_threshold = 0.50\n",
    "    results = [[i, r] for i, r in enumerate(res) if r > error_threshold]\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return_list = []\n",
    "\n",
    "    for r in results:\n",
    "        return_list.append({'intent': chatbotClasses[r[0]], 'probability': str(r[1])})\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other function that I created is called \"get_response\". This function is used to get a response for the user's input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(intents_list, intents_json):\n",
    "    tag = intents_list[0]['intent']\n",
    "    list_of_intents = intents_json['intents']\n",
    "    for i in list_of_intents:\n",
    "        if i['tag'] == tag:\n",
    "            result = random.choice(i['responses'])\n",
    "            return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "waist_exercises_gym = []\n",
    "back_exercises_gym = []\n",
    "cardio_exercises_gym = []\n",
    "chest_exercises_gym = []\n",
    "lower_arm_exercises_gym = []\n",
    "lower_leg_exercises_gym = []\n",
    "neck_exercises_gym = []\n",
    "shoulder_exercises_gym = []\n",
    "upper_arm_exercises_gym = []\n",
    "upper_leg_exercises_gym = []\n",
    "\n",
    "\n",
    "waist_exercises_calisthenics = []\n",
    "back_exercises_calisthenics = []\n",
    "cardio_exercises_calisthenics = []\n",
    "chest_exercises_calisthenics = []\n",
    "lower_arm_exercises_calisthenics = []\n",
    "lower_leg_exercises_calisthenics = []\n",
    "neck_exercises_calisthenics = []\n",
    "shoulder_exercises_calisthenics = []\n",
    "upper_arm_exercises_calisthenics = []\n",
    "upper_leg_exercises_calisthenics = []\n",
    "\n",
    "for exercise in exerciseData:\n",
    "    if exercise['bodyPart'] == 'back':\n",
    "        if exercise['equipment'] == 'body weight':\n",
    "            back_exercises_calisthenics.append(exercise)\n",
    "        else:\n",
    "            back_exercises_gym.append(exercise)\n",
    "    if exercise['bodyPart'] == 'cardio':\n",
    "        if exercise['equipment'] == 'body weight':\n",
    "            cardio_exercises_calisthenics.append(exercise)\n",
    "        else:\n",
    "            cardio_exercises_gym.append(exercise)\n",
    "    if exercise['bodyPart'] == 'chest':\n",
    "        if exercise['equipment'] == 'body weight':\n",
    "            chest_exercises_calisthenics.append(exercise)\n",
    "        else:\n",
    "            chest_exercises_gym.append(exercise)\n",
    "    if exercise['bodyPart'] == 'lower arms':\n",
    "        if exercise['equipment'] == 'body weight':\n",
    "            lower_arm_exercises_calisthenics.append(exercise)\n",
    "        else:\n",
    "            lower_arm_exercises_gym.append(exercise)\n",
    "    if exercise['bodyPart'] == 'lower legs':\n",
    "        if exercise['equipment'] == 'body weight':\n",
    "            lower_leg_exercises_calisthenics.append(exercise)\n",
    "        else:\n",
    "            lower_leg_exercises_gym.append(exercise)\n",
    "    if exercise['bodyPart'] == 'neck':\n",
    "        if exercise['equipment'] == 'body weight':\n",
    "            neck_exercises_calisthenics.append(exercise)\n",
    "        else:\n",
    "            neck_exercises_gym.append(exercise)\n",
    "    if exercise['bodyPart'] == 'shoulders':\n",
    "        if exercise['equipment'] == 'body weight':\n",
    "            shoulder_exercises_calisthenics.append(exercise)\n",
    "        else:\n",
    "            shoulder_exercises_gym.append(exercise)\n",
    "    if exercise['bodyPart'] == 'waist':\n",
    "        if exercise['equipment'] == 'body weight':\n",
    "            waist_exercises_calisthenics.append(exercise)\n",
    "        else:\n",
    "            waist_exercises_gym.append(exercise)\n",
    "    if exercise['bodyPart'] == 'upper arms':\n",
    "        if exercise['equipment'] == 'body weight':\n",
    "            upper_arm_exercises_calisthenics.append(exercise)\n",
    "        else:\n",
    "            upper_arm_exercises_gym.append(exercise)\n",
    "    if exercise['bodyPart'] == 'upper legs':\n",
    "        if exercise['equipment'] == 'body weight':\n",
    "            upper_leg_exercises_calisthenics.append(exercise)\n",
    "        else:\n",
    "            upper_leg_exercises_gym.append(exercise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last function that I created is called \"run_chatbot\". This function lets me run an instance of the chatbot after every call of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_chatbot(message):\n",
    "    ints = predict_class(message)\n",
    "    try:\n",
    "        response = get_response(ints, chatbotIntents)\n",
    "\n",
    "        if response == 'back_gym':\n",
    "            response = random.choice(back_exercises_gym)\n",
    "        if response == 'cardio_gym':\n",
    "            response = random.choice(cardio_exercises_gym)\n",
    "        if response == 'chest_gym':\n",
    "            response = random.choice(chest_exercises_gym)\n",
    "        if response == 'lower_arms_gym':\n",
    "            response = random.choice(lower_arm_exercises_gym)\n",
    "        if response == 'lower_legs_gym':\n",
    "            response = random.choice(lower_leg_exercises_gym)\n",
    "        if response == 'neck_gym':\n",
    "            response = random.choice(neck_exercises_gym)\n",
    "        if response == 'shoulders_gym':\n",
    "            response = random.choice(shoulder_exercises_gym)\n",
    "        if response == 'upper_arms_gym':\n",
    "            response = random.choice(upper_arm_exercises_gym)\n",
    "        if response == 'upper_legs_gym':\n",
    "            response = random.choice(upper_leg_exercises_gym)\n",
    "        if response == 'waist_gym':\n",
    "            response = random.choice(waist_exercises_gym)\n",
    "        if response == 'back_calisthenics':\n",
    "            response = random.choice(back_exercises_calisthenics)\n",
    "        if response == 'cardio_calisthenics':\n",
    "            response = random.choice(cardio_exercises_calisthenics)\n",
    "        if response == 'chest_calisthenics':\n",
    "            response = random.choice(chest_exercises_calisthenics)\n",
    "        if response == 'lower_arms_calisthenics':\n",
    "            response = random.choice(lower_arm_exercises_calisthenics)\n",
    "        if response == 'lower_legs_calisthenics':\n",
    "            response = random.choice(lower_leg_exercises_calisthenics)\n",
    "        if response == 'neck_calisthenics':\n",
    "            response = random.choice(neck_exercises_calisthenics)\n",
    "        if response == 'shoulders_calisthenics':\n",
    "            response = random.choice(shoulder_exercises_calisthenics)\n",
    "        if response == 'upper_arms_calisthenics':\n",
    "            response = random.choice(upper_arm_exercises_calisthenics)\n",
    "        if response == 'upper_legs_calisthenics':\n",
    "            response = random.choice(upper_leg_exercises_calisthenics)\n",
    "        if response == 'waist_calisthenics':\n",
    "            response = random.choice(waist_exercises_calisthenics)\n",
    "    except:\n",
    "        response = {\n",
    "        \"bodyPart\": \"N/A\",\n",
    "        \"equipment\": \"N/A\",\n",
    "        \"gifUrl\": \"https://www.iconpacks.net/icons/2/free-sad-face-icon-2691-thumb.png\",\n",
    "        \"id\": \"N/A\",\n",
    "        \"name\": \"N/A\",\n",
    "        \"target\": \"N/A\"\n",
    "    }\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining all those functions, I implemented a simple server using the Flask framework, so that I can run the chatbot on my local machine and I can connect the backend of the project to the frontend framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__' (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://localhost:3000/ (Press CTRL+C to quit)\n",
      "2022-04-07 16:28:25.185237: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "127.0.0.1 - - [07/Apr/2022 16:28:25] \"GET /?msg=can%20you%20advise%20me%20a%20lower%20arm%20workout%20for%20the%20home%20please HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [07/Apr/2022 16:28:42] \"GET /?msg=lower%20arm%20wrkout%20gym%20pls HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [07/Apr/2022 16:28:44] \"GET /?msg=lower%20arm%20wrkout%20gym%20pls HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "\n",
    "@app.route('/', methods=['GET'])\n",
    "def run_bot():\n",
    "    message = request.args.get('msg')\n",
    "    response = run_chatbot(message)\n",
    "    return response\n",
    "\n",
    "\n",
    "\n",
    "app.run(host='localhost', port=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "chatbot_implementation.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "61580b6376313b07636368374d502020b1c20037f44623e5351b987abb66b864"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('finalYearProjectEnv': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
